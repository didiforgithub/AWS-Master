import gradio as gr
import random
import time
import answer_generator_streaming as answer_generator

file_uploaded = False


# Chatbot demo with multimodal input (text, markdown, LaTeX, code blocks, image, audio, & video). Plus shows support for streaming text.

def add_text(documentation, text):
    documentation = documentation + [(text, None)]
    return documentation, gr.update(value="", interactive=False)


def add_file(history, file):
    history = history + [((file.name,), None)]
    return history


def bot(history):
    response = "**That's cool!**"
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history

def bool_upload_file():
    global file_uploaded
    if not file_uploaded:
        return [("User","Sorry I haven't upload zip file"),("Chatbot","please upload zip first")]
    else:
        pass

# åœ¨æµ‹è¯•ç‰ˆæœ¬çš„Chatbotä¸­ï¼Œæˆ‘ä»¬è¾“å…¥çš„æ˜¯ä¸æ–‡æ¡£ç›¸å…³çš„å†…å®¹ï¼Œå…¶ä½™çš„éƒ½æ˜¯ä½¿ç”¨demoè¿›è¡Œæµ‹è¯•çš„
def LLM_answer(documentation):
    bool_upload_file()
    answer = answer_generator.tutorial_generation_chain(
        {"project_structure": answer_generator.demo_project_structure, 
        "aws_documentation": documentation, 
        "tutorial_template": answer_generator.tutorial_template})
    
    for chunk in answer:
        cunrrent_chunk = ""
        for character in chunk:
            cunrrent_chunk += character
            time.sleep(0.05) # æ·»åŠ æ‰“å­—æ•ˆæœï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·çš„
            yield cunrrent_chunk

with gr.Blocks() as demo:
    chatbot = gr.Chatbot([], elem_id="chatbot",height=300)

    with gr.Row():
        with gr.Column(scale=0.85):
            txt = gr.Textbox(
                show_label=False,
                placeholder="upload the zip file ,and we'll tell you how to deploy it",
                container=False
            )
        with gr.Column(scale=0.15, min_width=0):
            btn = gr.UploadButton("ğŸ“", file_types=["zip"],type='bytes')

    # åœ¨æŒ‰é’®è¢«ç”µå‡»åˆ°çš„æ—¶å€™ï¼Œè§¦å‘Uploadæ–‡ä»¶ï¼Œä¼ é€’å‚æ•°ï¼Œåœ¨æ‰§è¡Œå®Œä¹‹åï¼Œä½¿ç”¨LLM answerä¼ é€’
    btn.upload(add_file,[chatbot,btn],[chatbot],queue = False).then(
        LLM_answer,chatbot,chatbot
    )
    # btn.click(LLM_answer, inputs=[txt], outputs=chatbot)

    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        LLM_answer, chatbot, chatbot
    )
    # txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=False)
    # file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
    #     bot, chatbot, chatbot
    # )

# å½“å‰çš„é€»è¾‘æ˜¯ï¼Œæäº¤æ–‡ä»¶ä¹‹åï¼Œå°±åº”è¯¥é©¬ä¸Šå‘LLMæ‰§è¡Œå‘½ä»¤è¯·æ±‚è¿”å›ç»“æœï¼ŒåŒæ—¶ä½¿ç”¨çš„æ˜¯yieldåšæµå¼è¾“å‡º
demo.queue()
if __name__ == "__main__":
    demo.launch(share=False)
