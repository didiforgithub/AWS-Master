# Chain of Thought 论文详解

[知乎详解COT开山之作](https://zhuanlan.zhihu.com/p/582758381)

## 前置知识
1. transformer 在语言模型上的架构有三种不同的设计模式：encoder-only(Bert)\encoder-decoder(Bart\T5)\decoder-only(GPT)。
2. 大规模语言模型所采用的都是 decoder-only 架构，并且进行的是自监督训练
3. SOTA
4. GPT3可以被分类为三种学习范式：few-shot\one-shot\zero-shot
   1. 少样本与零样本的唯一区别就是中间多出了一些参考样例，它们其实都是在续写前缀（只是零样本的输入没有任何参考，而少样本的输入有一些参考样例来帮助语言模型推断如何根据任务输入生成相应的任务输出）

### 使用一个训练好的大模型解决推理任务的几种范式
1. Zero-shot：直接输出
2. Zero-shot-COT：step by step
3. Manual COT：这种情况下使用到了少样本学习，在输入问题之前，手动设计一些问题和答案的样例（样例的答案给出中间推理步骤），这些问题和答案都需要手动构造，所以叫 Manual-CoT
4. Auto COT：通过多样性选取有代表性的问题。对于每一个采样的问题拼接上“Let's think step by step”（类似于 Zero-Shot-CoT ）输入到语言模型，让语言模型生成中间推理步骤和答案，然后把这些所有采样的问题以及语言模型生成的中间推理步骤和答案全部拼接在一起，构成少样本学习的样例，最后再拼接上需要求解的问题一起输入到语言模型中进行续写，最终模型续写出了中间的推理步骤以及答案
  1. 简单理解AutoCOT，就是在ManualCOT的基础上加上了一个Let's think step by step


### COT正文
背景：大型语言模型难以解决推理问题，这是因为大型语言模型主要的能力在于预测，而并非处理符号数字相关的推理任务

CoT定义：人类在遇到一系列问题时所产生的推理步骤，而它们的表现形式就是一系列的短句子（比如说在背景介绍中所提到的遇到数学问题时所产生的中间推理步骤）

传统的解决LLM推理困难的方法：
1. 针对下游任务对模型进行微调；
2. 为模型提供少量的输入输出样例进行学习（Standard Prompting）

#### 为什么COT能够解决问题

- CoT能够将一个问题拆分出多个中间步骤，这使得那些问题有机会得到更多的计算量。这是因为语言模型在生成输出的时候是一个一个 token 进行生成的，问题越难，CoT 使得生成的中间步骤越多，那么整体上生成的 token 的数量也会越多，质量也就更高
- CoT 提供了可解释性，也就是在不知道答案的情况下，也能够知道答案是怎样得来的，也就是所谓的中间推理步骤
- 作者认为 CoT 在原则上能够适用于任何人类能够用语言所能解决的问题，而不仅仅是数学、逻辑、常识这类的问题。因为 CoT 本身的载体就是一系列的短句子，本身也是人类语言